---

# ğŸ“ MCP â€“ SystÃ¨me de Tri Automatique de Dossiers (File Manager)

Ce projet utilise un pipeline basÃ© sur un **LLM (Ollama / Llama 3)** pour analyser, classer et organiser automatiquement des fichiers dans une arborescence logique.
Il fonctionne intÃ©gralement en local via Docker, ou peut se connecter Ã  Ollama Cloud si nÃ©cessaire.

---

## ğŸš€ Lancement rapide

### 1ï¸âƒ£ ExÃ©cuter le script PowerShell

```bash
.\run_file_manager.ps1 <chemin_du_dossier_a_trier>
```

Si vous avez une erreur liÃ©e Ã  l'exÃ©cution des scripts, activez les droits :

```bash
Set-ExecutionPolicy -Scope CurrentUser -ExecutionPolicy RemoteSigned
```

Voir les politiques actuelles :

```bash
Get-ExecutionPolicy -List
```

---

## ğŸ§  Installation de Ollama

### âœ” Installer Ollama (local)

Suivez lâ€™installation pour votre OS :
[https://ollama.com/download](https://ollama.com/download)

### âœ” TÃ©lÃ©charger un modÃ¨le local

Par exemple Llama 3 :

```bash
ollama pull llama3
```

### âœ” Utiliser le modÃ¨le automatiquement avec le MCP

Aucun changement Ã  faire : le script utilisera automatiquement Ollama.

---

## â˜ï¸ (OPTIONNEL) Utiliser Ollama Cloud

Si vous souhaitez exÃ©cuter les analyses cÃ´tÃ© cloud :

```bash
ollama signin
ollama serve
```

Puis sÃ©lectionnez un modÃ¨le cloud dans votre configuration. Ajouter ce model Ã  la ligne 76 du main.py.

---

# ğŸ›  Fonctionnement du pipeline

Le systÃ¨me suit 4 Ã©tapes principales :

---

## **1. RÃ©cupÃ©ration des fichiers**

Le script rÃ©cupÃ¨re :

* les noms de fichiers
* leurs chemins
* un extrait de leur contenu (prÃ©visualisation)

Cela permet au modÃ¨le de comprendre le type du document.

---

## **2. Analyse des fichiers**

Un premier traitement est effectuÃ© :

* extraction de mots-clÃ©s
* tentative dâ€™identification du type de fichier (CV, ordonnance, article, etc.)
* extraction de mÃ©tadonnÃ©es (dates, titres, noms)
* dÃ©tection de langue

Ces informations servent de base au LLM pour proposer un classement intelligent.

---

## **3. Classification via LLM**

Un prompt spÃ©cialisÃ© est envoyÃ© au LLM afin :

* de dÃ©terminer la catÃ©gorie exacte du fichier
* de proposer une **structure hiÃ©rarchique** cohÃ©rente
* de nommer les dossiers de maniÃ¨re propre
* dâ€™indiquer oÃ¹ chaque fichier doit Ãªtre dÃ©placÃ©

Le LLM retourne un **JSON strict**, par exemple :

```json
{
  "target_folder": "Documents/Articles/RÃ©seaux/2023",
  "keywords": ["network slicing", "VSR", "architecture"],
  "type": "article",
  "date": "2023-05-12"
}
```

---

## **4. DÃ©placement et crÃ©ation des dossiers**

Ã€ partir du JSON :

* les dossiers nÃ©cessaires sont crÃ©Ã©s automatiquement
* les fichiers sont dÃ©placÃ©s vers leur emplacement final
* les collisions de noms sont gÃ©rÃ©es
* les chemins sont sÃ©curisÃ©s

Le tri entier est **automatique**, reproductible, et pilotÃ© par le LLM.

---

# ğŸ“Œ RÃ©sumÃ© du workflow

```
[Fichiers brut]
       â†“
[Extraction keywords + mÃ©tadonnÃ©es]
       â†“
[LLM â†’ propose une hiÃ©rarchie complÃ¨te]
       â†“
[CrÃ©ation dossiers + tri automatique]
       â†“
[Dossier final organisÃ© proprement]
```

---

# ğŸ“ Notes

* Aucun fichier nâ€™est supprimÃ© automatiquement.
* Le systÃ¨me fonctionne en local : vos documents ne quittent jamais votre machine.
* Le modÃ¨le recommandÃ© est **Llama 3 (via Ollama)**, performant pour classification.
* Le script fonctionne sous Windows, Linux et macOS via Docker.


erreur ajouter des outils 
serveur definit des outils 
clients choisit ce quil veut faire  
api chat => in formation serveur => besoin d'appeler => appeler telle fonction